# よくある失敗例

モニタリング・アラート設計でよく見られる失敗パターンと対策を紹介します。

## 1. 全てをCriticalにする

### 問題

全てのアラートを最高優先度にすることで、アラート疲れ（Alert Fatigue）が発生します。

**症状:**

- 大量のアラートに埋もれて、本当に重要なアラートを見逃す
- チームメンバーがアラートを無視するようになる
- オンコール対応の負担が大きすぎる

**失敗例:**

```
❌ S3バケット容量増加 → Critical（即時対応）
❌ EC2 CPU 70%超過 → Critical（即時対応）
❌ Lambda実行時間が平均より長い → Critical（即時対応）
```

### 対策

エンドユーザー影響を基準に優先度を設定します。

```
✅ API Gateway 5xxエラー率 > 5% → Critical（即時対応）
✅ EC2 CPU 80%超過 → Warning（週次レビュー時確認）
✅ Lambda実行時間が平均より長い → Info（ダッシュボード確認）
```

**判断基準:**
- エンドユーザーに影響がある → Critical
- 予兆段階 → Warning
- 傾向把握 → Info

## 2. 単層のみの監視

### 問題

一つのレイヤーだけを監視することで、問題の見逃しや原因特定の遅れが発生します。

**失敗例:**

```
❌ Syntheticsだけを監視
   → 一部ターゲットがUnhealthyでも検知できない

❌ ALBのTargetHealthだけを監視
   → エンドユーザー視点でのサービス品質が不明

❌ アプリケーションログだけを監視
   → インフラ起因の問題を見逃す
```

### 対策

多層的に監視し、各レイヤーで異なる視点を持ちます。

```
✅ ユーザー体験: Synthetics、RUM
✅ アプリケーション: API Gateway、Lambda、X-Ray
✅ インフラ: ALB TargetHealth、RDS、DynamoDB
```

**ポイント:**
- 各レイヤーは重複ではなく補完関係
- 上位レイヤーで監視していても、下位レイヤーの監視は必要

## 3. Auto Scaling環境で固定閾値を使用

### 問題

Auto Scalingが動作する環境で固定的なリソース閾値を使うと、誤検知が多発します。

**失敗例:**

```
❌ EC2インスタンス数が2台未満 → Critical
   （Auto Scalingでスケールダウンしただけ）

❌ CPU 50%超過 → Critical
   （Auto Scalingが正常に機能している範囲内）
```

### 対策

Auto Scalingの動作を考慮した監視をします。

```
✅ Desired Capacity != 実際のインスタンス数 → Critical
✅ Auto Scaling アクティビティの失敗 → Critical
✅ CPU 80%超過かつAuto Scaling失敗 → Critical
✅ CPU 80%超過（Auto Scaling正常） → Warning（週次レビュー）
```

**ポイント:**
- Auto Scalingが正常なら、リソース使用率の上昇は問題ない
- Auto Scaling自体の動作を監視する

## 4. WarningやInfoに過度に迅速な対応を求める

### 問題

ユーザー影響のないアラートに即座の対応を求めることで、開発生産性が低下します。

**失敗例:**

```
❌ EC2 CPU 80%超過 → 数分以内に対応
❌ S3バケット容量増加 → 即日対応
❌ Lambda実行時間増加 → 数時間以内に対応
```

**症状:**

- 継続的な割り込みで集中できない
- 本当に重要なCriticalアラートへの感度が下がる
- 持続可能な運用ができない

### 対策

ユーザー影響がないアラートは、基本的に週次/月次レビュー時の対応とします。

```
✅ EC2 CPU 80%超過 → 週次レビュー時に傾向を確認
✅ S3バケット容量増加 → 月次レビュー時にコスト最適化を検討
✅ Lambda実行時間増加 → 週次レビュー時にコード改善を検討
```

**例外:**
- 急成長中のサービス
- イベント前（セール、キャンペーン）
- 過去に問題があったシステム

## 5. 閾値が低すぎる

### 問題

閾値が低すぎると誤検知が多発し、アラートの信頼性が低下します。

**失敗例:**

```
❌ CPU > 50%で即座にアラート
❌ レスポンスタイムが平均より1ms遅いだけでアラート
❌ エラー率 > 0.01%でアラート
```

**症状:**

- 正常な動作範囲内でアラートが発火
- アラートが多すぎて重要なものを見逃す

### 対策

過去のデータを分析し、適切な閾値を設定します。

```
✅ 過去30日間のデータを分析
✅ 通常時の最大値 + マージンで閾値を設定
✅ 定期的に閾値を見直す
✅ システム特性に応じて調整
```

## 6. 評価期間が短すぎる

### 問題

評価期間が短いと、一時的なスパイクで不要なアラートが発生します。

**失敗例:**

```
❌ CPU > 90%を1回観測しただけでアラート
❌ エラーが1件発生しただけでアラート
```

### 対策

適切な評価期間を設定します。

```
✅ CPU > 90%が5分間継続 → アラート
✅ エラー率 > 5%が5分間継続 → アラート
✅ 3回連続で閾値超過 → アラート
```

## 7. InfoレベルをSNS通知にする

### 問題

傾向把握のためのInfoレベルをSNS通知にすると、大量の通知でSlackやメールが埋まります。

**失敗例:**

```
❌ S3バケット容量が1GB増加 → Slack通知
❌ Lambda実行時間が平均より0.1秒長い → メール通知
❌ 全てのメトリクスをSlackに通知
```

**症状:**

- Slackやメールが通知で埋まる
- 本当に重要な情報が埋もれる
- チームメンバーが通知を無視するようになる

### 対策

Infoレベルはダッシュボードで定期確認する運用にします。

```
✅ Infoレベル → ダッシュボード表示（通知なし）
✅ 朝会や週次レビューでダッシュボードを確認
✅ 長期的な傾向を把握する
```

## 8. システム特性を無視した一律の閾値

### 問題

異なる特性のシステムに同じ閾値を適用すると、誤検知や見逃しが発生します。

**失敗例:**

```
❌ 全てのシステムでCPU 80%を閾値にする
❌ トラフィックパターンを無視した固定閾値
❌ バッチ処理実行中も同じ閾値を適用
```

### 対策

システムごとに特性を理解し、適切な閾値を設定します。

```
✅ ピーク時とオフピーク時で異なる閾値
✅ バッチ処理実行中は閾値を調整
✅ システムごとにトラフィックパターンを分析
✅ 閾値は定期的に見直し
```

## 9. エスカレーションパスがない

### 問題

アラートが発火しても誰が対応すべきか不明で、対応が遅れます。

**失敗例:**

```
❌ 全てのアラートが同じチャンネルに通知
❌ 誰が対応すべきか不明
❌ エスカレーションの仕組みがない
```

### 対策

明確なエスカレーションパスを設計します。

```
✅ Critical → オンコールエンジニア（PagerDuty）
✅ 15分以内に対応開始されない場合、上位者へエスカレーション
✅ Warning → 運用チーム（Slack）
✅ Info → ダッシュボード確認
```

## 10. レビューと改善のサイクルがない

### 問題

一度設定したアラートを見直さないことで、環境変化に対応できません。

**失敗例:**

```
❌ アラート設定を一度決めたら放置
❌ 誤検知が多発しても見直さない
❌ システム構成変更時にアラートを更新しない
```

### 対策

定期的にレビューし、継続的に改善します。

```
✅ Criticalアラート発生時に適切だったか評価
✅ 週次/月次レビューでWarningアラートを確認
✅ 誤検知の頻度を確認し、閾値を調整
✅ システム構成変更時にアラートを見直し
```

## まとめ

よくある失敗の多くは、以下の原則を守ることで防げます:

1. **エンドユーザー影響を基準にする**
2. **多層的に監視する**
3. **Auto Scalingの動作を理解する**
4. **適切な対応時間を設定する**（Criticalは即時、WarningやInfoは週次/月次）
5. **過去のデータを分析して閾値を設定する**
6. **システム特性に応じて柔軟に調整する**
7. **定期的にレビューし、改善を続ける**

これらの原則を守ることで、持続可能で効果的なモニタリング・アラート運用が実現できます。
